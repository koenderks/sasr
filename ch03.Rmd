---
output:
  bookdown::html_document2:
    fig_caption: yes
editor_options:
  chunk_output_type: console
---

```{r echo = FALSE, cache = FALSE}
source("utils.R", local = TRUE)
```

# Planning a Sample {#CHAPTER-3}

<p align='center'><img src='https://github.com/koenderks/jfa/raw/development/vignettes/img/planning.png' alt='planning' width='100%'></p>

One of the key considerations in audit sampling is determining the minimum
sample size required to achieve a desired level of assurance or precision. In
this chapter, we will discuss how to use three standard likelihoods to plan a
minimum sample size for audit sampling: the hypergeometric likelihood, the
binomial likelihood and the Poisson likelihood.

Before we dive into the details of computing a minimum sample size for audit
sampling using any of the three likelihoods, it is important to review some key
concepts and notation. Let's start by defining the population and the sample.
The population is the group of items or transactions being audited, and the
sample is the subset of the population that is selected for testing. The
population size is denoted by $N$, and the sample size is denoted by $n$.

Next, we need to define the maximum tolerable error or precision level that we
want to achieve in our audit. This is typically expressed as a maximum allowable
misstatement rate, denoted by $\theta_{max}$. For example, if we set
$\theta_{max}$ to 0.05, it means that we want to ensure that the misstatement
rate in the population does not exceed 5%.

## Required Information

First, planning a minimum sample requires knowledge of the conditions that lead 
to acceptance or rejection of the population (i.e., the sampling objectives).
Typically, sampling objectives can be classified into one or both of the
following:

- **Hypothesis testing**: The goal of the sample is to obtain evidence for or
  against the claim that the misstatement in the population is lower than a
  given value (i.e., the performance materiality).
- **Estimation**: The goal of the sample is to obtain an accurate estimate of
  the misstatement in the population (with a minimum precision).

Second, it is advised to specify the expected (or tolerable) misstatements in
the sample. The expected misstatements are the misstatements that you allow in
the sample, while still retaining the desired amount of assurance about the 
population. It is strongly recommended to set the value for the expected
misstatements in the sample conservatively to minimize the chance of the
observed misstatements in the sample exceeding the expected misstatements, which
would imply that insufficient work has been done in the end.

Finally, next to determining the sampling objective(s) and the expected
misstatements, it is important to determine the statistical distribution linking
the sample outcomes to the population misstatement. This distribution is called
the likelihood (i.e., `poisson`, `binomial`, or`hypergeometric`). All three
aforementioned likelihoods are commonly used in an audit sampling context,
however, `poisson` is the default likelihood in **jfa** because it is the most
conservative of the three. In the subsections below, we elaborate on the three
standard likelihoods for audit sampling and demonstrate how they can be used
to obtain a minimum sample size.

## The Hypergeometric Likelihood

Let's consider how to use the hypergeometric likelihood to calculate the
minimum sample size needed to achieve a desired level of assurance. The
hypergeometric distribution is a discrete probability distribution that is
commonly used to model the number of events occurring in a fixed number of
trials when the population size is known. For our purpose, we can use the
hypergeometric distribution as a likelihood to model the number of misstatements
that are expected to be found in the sample.

The probability mass function (PMF) of the hypergeometric distribution is given
by:

\begin{equation}
  p(X=x)=\frac{\binom{K}{x}\binom{N-K}{n-x}}{\binom{N}{n}},
\end{equation}

where $x$ is the number of misstatements in the sample, $n$ is the sample size,
$N$ is the population size and $K$ is the total number of misstatements assumed
in the population. The assumed misstatements $K$ is a linear extrapolation of
the assumed misstatement rate in the population $\theta_{max}$ to the total
population:

\begin{equation}
  K = \theta_{max} N.
\end{equation}

### Classical planning

Concretely, the following statistical model is assumed:

\begin{equation}
  x \sim \text{Hypergeometric}(n, N, K)
\end{equation}

Given a desired misstatement tolerance $\theta_{max}$, we can solve for the
minimum sample size $n$ needed to achieve this assurance level. In **jfa**,
this sample size can be calculated using the `planning()` function. For example,
if we want to achieve an assurance level of 95% ($\alpha=0.05$) for a
maximum tolerable misstatement rate of 3% in a population of $N=1000$, then the
required sample size under the assumption of zero expected misstatements in the
sample is $n = 94$.

```{r}
planning(materiality = 0.03, expected = 0, conf.level = 0.95,
         likelihood = "hypergeometric", N.units = 1000)
```

As another example, if we want to achieve an assurance level of 95%
($\alpha=0.05$) for a maximum tolerable misstatement rate of 3% in a population
of $N=1000$, then the required sample size under the assumption of one expected
misstatement in the sample is $n = 147$.

```{r}
planning(materiality = 0.03, expected = 1, conf.level = 0.95,
         likelihood = "hypergeometric", N.units = 1000)
```

### Bayesian Planning

Performing Bayesian planning with the hypergeometric likelihood requires that
you specify a prior distribution for the parameter $\theta$. Practically, this
means that you should provide an input for the `prior` argument in the
`planning()` function.

Setting `prior = TRUE` performs Bayesian planning using a [default prior](https://koenderks.github.io/jfa/articles/creating-prior.html#default-priors-method-default)
conjugate to the specified `likelihood` (i.e., a beta-binomial prior).
Concretely, this means that the following statistical model is assumed:

\begin{align}
  x \sim& \text{Hypergeometric}(n, N, K) \\
  \theta &\sim \text{Beta-binomial)(\alpha, \beta)
\end{align}

> The beta-binomial prior is conjugate to the hypergeometric likelihood, which
means that the posterior distribution of $\theta$ can be determined
analytically.

For example, the command below uses a default beta-binomial(N, 1, 1) prior
distribution to plan the sample, since `planning()` is given the hypergeometric
likelihood. If we want to achieve an assurance level of 95% ($\alpha=0.05$) for
a maximum tolerable misstatement rate of 3% in a population of $N=1000$, then
the required sample size under the assumption of zero expected misstatements in
the sample is $n = 93$.

```{r}
plan <- planning(materiality = 0.03, expected = 0, conf.level = 0.95,
                 likelihood = "hypergeometric", N.units = 1000, prior = TRUE)
summary(plan)
```

You can inspect how the prior distribution compares to the expected
posterior distribution by using the `plot()` function. The expected posterior
distribution is the posterior distribution that would occur if you actually
observed the planned sample containing the expected misstatements.

```{r}
plot(plan)
```

The hypergeometric likelihood does not allow for non-conjugate prior
distributions to be used as a prior.

## The Binomial Likelihood

Let's consider how to use the binomial likelihood to calculate the minimum
sample size needed to achieve a desired level of assurance. The binomial
distribution is a discrete probability distribution that is commonly used to
model the number of events occurring in a fixed number of trials. For our
purpose, we can use the binomial distribution as a likelihood to model the
number of misstatements that are expected to be found in the sample. 

> In audit sampling, the binomial likelihood is often used to approximate the
hypergeometric likelihood since it is easier to work with (i.e., it only has two
parameters: $\theta$ and $n$, while the hypergeometric has three: $n$, $N$, and
$K$). However, the binomial likelihood is more conservative than the
hypergeometric likelihood, meaning that resulting sample sizes will be higher.

The probability mass function (PMF) of the binomial distribution is given by:

\begin{equation}
  p(x; n, \theta) = \binom{n, x} \theta^{x} (1-\theta)^{n - x},
\end{equation}

where $x$ is the number of misstatements in the sample, $n$ is the sample size
and $\theta$ is the misstatement rate expected in the sample.

### Classical Planning

Concretely, the following statistical model is assumed:

\begin{equation}
  x \sim \text{Binomial}(n, \theta_{max})
\end{equation}

Given a desired misstatement tolerance $\theta_{max}$, we can solve for
the minimum sample size $n$ needed to achieve the desired assurance level. A
useful trick to utilize is that, if we do not expect any misstatements in the
sample, the formula for the minimum required sample size reduces to:

\begin{equation}
  n = \lceil\frac{\ln(\alpha)}{\ln(1 - \theta_{max})}\rceil.
\end{equation}

> $\lceil...\rceil$ is the ceiling function. Hence, $\lceil1.2\rceil = 2$.

For example, if we want to achieve an assurance level of 95% ($\alpha=0.05$) for
a maximum tolerable misstatement rate of 3%, then the required sample size under
the assumption of zero expected misstatements in the sample is $n = 99$.

```{r}
ceiling(log(1 - 0.95) / log(1 - 0.03))
```

In **jfa**, this sample size can be replicated using the `planning()` function.

```{r}
planning(materiality = 0.03, expected = 0, conf.level = 0.95,
         likelihood = "binomial")
```

However, if the number of expected misstatements in the sample is non-zero,
it becomes more difficult to solve the formula for $n$. Hence, we can
iteratively try every value of $n$ and return the smallest integer that
satisfies the sampling objectives. In **jfa**, this can be done by adjusting the
`expected` argument in the `planning()` function. For example, if we want to
achieve an assurance level of 95% ($\alpha=0.05$) for a maximum tolerable
misstatement rate of 3%, then the required sample size under the assumption of
one expected misstatement in the sample is $n = 157$.

```{r}
planning(materiality = 0.03, expected = 1, conf.level = 0.95,
         likelihood = "binomial")
```

### Bayesian Planning

Performing Bayesian planning with the binomial likelihood requires that you
specify a prior distribution for the parameter $\theta$. Practically, this means
that you should provide an input for the `prior` argument in the `planning()`
function.

Setting `prior = TRUE` performs Bayesian planning using a [default prior](https://koenderks.github.io/jfa/articles/creating-prior.html#default-priors-method-default)
conjugate to the specified `likelihood` (i.e., a beta prior). Concretely, this
means that the following statistical model is assumed:

\begin{align}
  x \sim& \text{Binomial}(n, \theta) \\
  \theta &\sim \text{Beta}(\alpha, \beta)
\end{align}

> The beta prior is conjugate to the binomial likelihood, which means that the
posterior distribution of $\theta$ can be determined analytically.

For example, the command below uses a default beta($\alpha=1$, $\beta=1$) prior
distribution to plan the sample, since `planning()` is given the binomial
likelihood. If we want to achieve an assurance level of 95% ($\alpha=0.05$) for
a maximum tolerable misstatement rate of 3%, then the required sample size under
the assumption of zero expected misstatements in the sample is $n = 98$.

```{r}
plan <- planning(materiality = 0.03, expected = 0, conf.level = 0.95,
                 likelihood = "binomial", prior = TRUE)
summary(plan)
```

You can inspect how the prior distribution compares to the expected
posterior distribution by using the `plot()` function. The expected posterior
distribution is the posterior distribution that would occur if you actually
observed the planned sample containing the expected misstatements.

```{r}
plot(plan)
```

The input for the `prior` argument can also be an object created by the
`auditPrior` function. If `planning()` receives a prior for which there is no
conjugate likelihood available, it will numerically derive the posterior
distribution. For example, the command below uses a Normal(0, 0.05) prior
distribution to plan the sample using the binomial likelihood. Concretely, this
means that the following statistical model is assumed:

\begin{align}
  x &\sim \text{Binomial}(n, \theta) \\
  \theta &\sim \text{Normal}(\mu = 0, \sigma = 0.05)
\end{align}

```{r}
prior <- auditPrior(method = "param", likelihood = "normal",
                    alpha = 0, beta = 0.05)

plan <- planning(materiality = 0.03, expected = 0, conf.level = 0.95,
                 likelihood = "poisson", prior = prior)

summary(plan)
```

The resulting sample size under this prior is $n = 90$, a reduction of 8 samples
when compared to the default beta(1, 1) prior distribution.

```{r}
plot(plan)
```

## The Poisson Likelihood

Let's consider how to use the Poisson likelihood to calculate the minimum
sample size needed to achieve a desired level of assurance. The Poisson
distribution is a discrete probability distribution that is commonly used to
model the number of events occurring in a fixed time or space. We can use the
Poisson distribution as a likelihood to model the number of misstatements that
are expected to be found in the sample.

> In audit sampling, the Poisson likelihood is often used to approximate the
binomial likelihood since it is easier to work with (i.e., it only has one
parameter: $\lambda$, while the binomial has two parameters: $\theta$ and $n$).
However, the Poisson likelihood is more conservative than the binomial
likeliood, meaning that resulting sample sizes will be higher.

The probability mass function (PMF) of the Poisson distribution is given by:

\begin{equation}
  p(x;\lambda) = \frac{\lambda^x e^{-\lambda}}{x!},
\end{equation}

where $x$ is the number of misstatements in the sample, and $\lambda$ is the
average number of misstatements expected in the sample. The average number of
misstatements is related to the misstatement rate in the population, denoted by
$\theta$, and the sample size, $n$, by the following equation:

\begin{equation}
  \lambda=n\theta.
\end{equation}

### Classical planning

Concretely, the following statistical model is assumed:

\begin{equation}
  x \sim \text{Poisson}(n\theta_{max})
\end{equation}

Given a desired misstatement tolerance $\theta_{max}$ and the Poisson
likelihood, we can solve for the minimum sample size $n$ needed to achieve a
assurance level. A useful trick to utilize is that, if we do not expect any
misstatements in the sample, the formula for the required sample size reduces
to:

\begin{equation}
  n = \lceil-\frac{\ln(\alpha)}{\theta_{max}}\rceil.
\end{equation}

> $\lceil...\rceil$ is the ceiling function. Hence, $\lceil1.2\rceil = 2$.

For example, if we want to achieve an assurance level of 95% ($\alpha=0.05$) for
a maximum tolerable misstatement rate of 3%, then the required sample size under
the assumption of zero expected misstatements in the sample is $n = 100$.

```{r}
ceiling(-log(1 - 0.95) / 0.03)
```

In **jfa**, this sample size can be replicated using the `planning()` function.

```{r}
planning(materiality = 0.03, expected = 0, conf.level = 0.95,
         likelihood = "poisson")
```

However, if the number of expected misstatements in the sample is non-zero,
it becomes more difficult to solve the formula for $n$. Hence, we can
iteratively try every value of $n$ and return the smallest integer that
satisfies the sampling objectives. In **jfa**, this can be done by adjusting the
`expected` argument in the `planning()` function. For example, if we want to
achieve an assurance level of 95% ($\alpha=0.05$) for a maximum tolerable
misstatement rate of 3%, then the required sample size under the assumption of
one expected misstatement in the sample is $n = 159$.

```{r}
planning(materiality = 0.03, expected = 1, conf.level = 0.95,
         likelihood = "poisson")
```

### Bayesian Planning

Performing Bayesian planning with the Poisson likelihood requires that you
specify a prior distribution for the parameter $\theta$. Practically, this means
that you should provide an input for the `prior` argument in the `planning()`
function.

Setting `prior = TRUE` performs Bayesian planning using a [default prior](https://koenderks.github.io/jfa/articles/creating-prior.html#default-priors-method-default)
conjugate to the specified `likelihood` (i.e., a gamma prior). Concretely, this
means that the following statistical model is assumed:

$$x \sim \text{Poisson}(n\theta)$$
$$\theta \sim \text{Gamma}(\alpha, \beta)$$

> The gamma prior is conjugate to the Poisson likelihood, which means that the
posterior distribution of $\theta$ can be determined analytically.

For example, the command below uses a default gamma($\alpha=1$, $\beta=1$) prior
distribution to plan the sample, since `planning()` is given the Poisson
likelihood. If we want to achieve an assurance level of 95% ($\alpha=0.05$) for
a maximum tolerable misstatement rate of 3%, then the required sample size under
the assumption of zero expected misstatements in the sample is $n = 99$.

```{r}
plan <- planning(materiality = 0.03, expected = 0, conf.level = 0.95,
                 likelihood = "poisson", prior = TRUE)
summary(plan)
```

You can inspect how the prior distribution compares to the expected
posterior distribution by using the `plot()` function. The expected posterior
distribution is the posterior distribution that would occur if you actually
observed the planned sample containing the expected misstatements.

```{r}
plot(plan)
```

The input for the `prior` argument can also be an object created by the
`auditPrior` function. If `planning()` receives a prior for which there is no
conjugate likelihood available, it will numerically derive the posterior
distribution. For example, the command below uses a Normal(0, 0.05) prior
distribution to plan the sample using the Poisson likelihood. Concretely, this
means that the following statistical model is assumed:

$$x \sim \text{Poisson}(n\theta)$$
$$\theta \sim \text{Normal}(\mu = 0, \sigma = 0.05)$$

```{r}
prior <- auditPrior(method = "param", likelihood = "normal",
                    alpha = 0, beta = 0.05)

plan <- planning(materiality = 0.03, expected = 0, conf.level = 0.95,
                 likelihood = "poisson", prior = prior)

summary(plan)
```

The resulting sample size under this prior is $n = 91$, a reduction of 8 samples
when compared to the default gamma(1, 1) prior.

```{r}
plot(plan)
```
