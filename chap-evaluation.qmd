```{r echo = FALSE, cache = FALSE}
source("utils.R", local = TRUE)
```

::: {.callout-note appearance="simple"}
You are reading the work-in-progress first edition of Statistical Auditing with R. This chapter is currently a dumping ground of ideas, and it is incomplete.
:::

# Evaluation {#chap-evaluation}

In statistical audit sampling, the evaluation of the sample is the crucial last step in the process. Here, the auditor assesses the audit evidence collected from the sample and blends it with the evidence gathered in earlier stages of the audit. The outcome of this evaluation forms the auditor's overall opinion of the population being audited.

Just as with planning a sample, evaluating a sample requires knowledge of the circumstances that determine whether the population should be accepted or rejected, which are referred to as sampling objectives. Sampling objectives can be divided into two main categories:

- **Hypothesis testing**: The goal of the sample is to obtain evidence for or against the claim that the misstatement in the population is lower than a given value (i.e., the performance materiality).
- **Estimation**: The goal of the sample is to obtain an accurate estimate of the misstatement in the population with a certain precision.

![When evaluating a sample with respect to a specific hypothesis, the auditor must consider the evidence in favour as well as the evidence against that hypothesis. Image available under a [CC-BY-NC 4.0](https://creativecommons.org/licenses/by-nc/4.0/legalcode) license.](img/book_justice.png){fig-align="center" width=75%}

When an auditor needs to perform audit sampling, they typically have two options: non-stratified or stratified sampling. Non-stratified sampling involves selecting a sample of units without considering any categorical characteristics, such as product type or store location, of these items. Stratified sampling involves dividing the population into subgroups based on specific characteristics and selecting a sample from each subgroup. In comparison to non-stratified sampling, stratified sampling can improve efficiency. However, in this chapter, we will focus on non-stratified audit sampling, while stratified audit sampling will be covered in the next chapter.

Non-stratified sampling is typically used when the population is considered homogenous, meaning there are no significant differences between subgroups. This approach is also suitable when the auditor does not need to consider differences between subgroups. For example, when an auditor reviews a company's inventory using non-stratified sampling, they may choose a random sample of items from the entire inventory without dividing it into subgroups. Similarly, when conducting an audit of a small business's general ledger, an auditor may select a sample of entries without dividing them based on categorical features such as payment method.

To get started with non-stratified evaluation of audit samples, the auditor must first decide if they want to evaluate based on summary statistics derived from a sample (i.e., 1 misstatement in 100 items) or if they want to calculate the misstatements from a data set. To make the principle of evaluation easier to understand, we will first explain how to evaluate a sample using summary statistics from a sample.

## Classical Evaluation

In classical evaluation, confidence intervals and *p*-values are used to measure the uncertainty and the evidence against the hypothesis of intolerable misstatement, respectively.

Confidence intervals play a crucial role in classical inference by helping to determine the uncertainty in a sample estimate. For example, if an auditor needs to estimate the misstatement in a tax return, they can calculate a confidence interval for the misstatement using classical inference. This confidence interval represents a range of possible values in which the true misstatement of the population is likely to fall. This range helps auditors make informed decisions about the misstatement and determine the potential impact of the misstatement on the (loss of) taxes.

To illustrate, suppose an auditor wants to estimate the misstatement in a population based on a sample of 100 items containing one misstatement. Using the `evaluation()` function in **jfa** and specifying `x = 1` and `n = 100`, the output shows that the estimated most likely misstatement in the population is 1 percent, and the 95 percent (one-sided) confidence interval ranges from 0 percent to 4.74 percent. It is important to note that the correct interpretation of a 95 percent confidence interval is: *"If we were to repeat the experiment over and over, then 95 percent of the time the confidence interval contains the true misstatement rate"* [@Hoekstra2014].

```{r}
evaluation(x = 1, n = 100, method = "binomial")
```

Classical hypothesis testing relies on the *p*-value to determine whether to accept or reject a certain hypothesis about a population. For example, suppose an auditor wishes to test whether the population contains misstatements of less than 3 percent (they formulate the performance materiality based on existing rules and regulations). They would create the hypotheses $H_1:\theta<0.03$ and $H_0:\theta\geq0.03$. The significance level is set to 0.05, equivalent to an audit risk of 5 percent. This means that a *p*-value below 0.05 is sufficient to reject the hypothesis of intolerable misstatement $H_0$.

In **jfa**, a classical hypothesis test using the *p*-value can be conducted by specifying the `materiality` argument in the `evaluation()` function. For example, to indicate a performance materiality of 3 percent, the auditor can specify `materiality = 0.03`. Along with the confidence interval, the output displays a *p*-value of 0.19462, which is greater than 0.05. Therefore, the hypothesis $H_0$ cannot be rejected at a significance level of 5 percent. As a result, the auditor cannot conclude that the sample provides sufficient evidence to reduce the audit risk to an appropriate level and cannot state that the population does not have misstatements of 3 percent or more.

```{r}
eval <- evaluation(materiality = 0.03, x = 1, n = 100, method = "binomial")
eval
```

The exact definition of the *p*-value is *"the probability of observing the data, or more extreme data, given the truth of the hypothesis of intolerable misstatement"*. The *p*-value of 0.19462 can be visualized via the `plot()` function.

```{r}
plot(eval)
```

## Bayesian Evaluation

In classical inference, we use confidence intervals to quantify the uncertainty of our estimate. In Bayesian inference, we use credible intervals for this purpose. To do Bayesian inference, we need to specify a prior distribution, which is our best guess about what the misstatement might look like before we actually collect any data. Then, we combine this prior with the information from our sample to get a posterior distribution.

We can use this posterior distribution to estimate the most likely misstatement in the population and the range of values that we are 95 percent confident the true value lies within. The classical confidence interval is often misinterpreted as a Bayesian credible interval. That is, a Bayesian 95 percent credible interval can be intuitively interpreted as: *"There is a 95 percent probability that the misstatement falls within the credible interval"*.

For example, if we have a uniform *beta(1, 1)* prior, sample of 100 units and we find that one contains a misstatement, we can estimate that the most likely misstatement in the population is 1 percent. Using the posterior distribution, we can calculate that there is a 95 percent probability that the true misstatement rate is between 0 percent and 4.61 percent. The small difference between the classical and default Bayesian results is due to the uniform beta(1, 1) prior distribution. Classical results can be emulated by constructing a prior with `method = "strict"` in the `auditPrior()` function. Remember that any call to `evaluation()` can be done in a Bayesian way by specifying a prior distribution. Hence, the only difference between the call for a classical analysis and the call for a Bayesian analysis is the use of a `prior` constructed via a call to `auditPrior()`.

```{r}
prior <- auditPrior(materiality = 0.05, method = "default", likelihood = "binomial")
eval <- evaluation(x = 1, n = 100, prior = prior)
eval
```

The posterior distribution with the most likely misstatement and credible interval can be visualized with the `plot()` function.

```{r}
plot(eval)
```

Classical inference relies on *p*-values to assess the strength of evidence against the hypothesis of intolerable misstatement, while Bayesian inference uses the Bayes factor to measure evidence in support or against the hypotheses of tolerable or intolerable misstatement. Bayesian hypothesis testing uses the Bayes factor, either $BF_{10}$ or $BF_{01}$, to draw conclusions about the evidence provided by the sample in favor of one of the two hypotheses, $H_1$ or $H_0$. 

The Bayes factor is an intuitive measure of statistical evidence that allows auditors to interpret the likelihood of the data occurring under either hypothesis. For example, if the value of $BF_{10}$ is 10 (as outputted by the `evaluation()` function), it means that the data are 10 times more likely to have arisen under $H_1$ than under $H_0$. A Bayes factor $BF_{10}$ greater than 1 indicates evidence for $H_1$ and against $H_0$, while a Bayes factor $BF_{10}$ less than 1 indicates evidence for $H_0$ and against $H_1$. The `evaluation()` function returns $BF_{10}$ by default, but $BF_{01}$ can be computed as the inverse of $BF_{10}$ (i.e., $\frac{1}{BF_{10}}$).

Let's take the example of an auditor who wants to verify whether a population contains less than 5 percent misstatement, which corresponds to the hypotheses $H_1:\theta<0.05$ and $H_0:\theta\geq0.05$. The auditor has taken a sample of 100 items, and only one was found to contain a misstatement. Assuming a default *beta(1,1)* prior distribution, the following code evaluates the sample using a Bayesian hypothesis test and the Bayes factor. The `materiality = 0.05` argument specifies the materiality for this audit.

```{r}
evaluation(materiality = 0.05, x = 1, n = 100, prior = prior)
```

The Bayes factor for this case is $BF_{10}=515$, indicating that the sample data is 515 times more likely to occur under the hypothesis of tolerable misstatement than under the hypothesis of material misstatement. To calculate this Bayes factor, we use the prior distribution and the posterior distribution. The Bayes factor is defined as the change from prior-to-posterior odds. To begin, we calculate the prior probability of the hypothesis of tolerable misstatement under the *beta(1,1)* prior distribution.

```{r}
prior.prob.h1 <- pbeta(0.05, shape1 = 1, shape2 = 1)
prior.prob.h1
```

The probability of intolerable misstatement hypothesis is the complement of the probability of tolerable misstatement hypothesis. To put it simply, it is just one minus the prior probability of the hypothesis of tolerable misstatement.

```{r}
prior.prob.h0 <- 1 - prior.prob.h1
prior.prob.h0
```

The prior odds is defined as the ratio of the prior probabilities.

```{r}
prior.odds.h1 <- prior.prob.h1 / prior.prob.h0
prior.odds.h1
```

To compute the posterior probability of the hypothesis of tolerable misstatement, we can use the posterior distribution. Additionally, we can obtain the posterior probability of the hypothesis of intolerable misstatement by subtracting the posterior probability of tolerable misstatement from one. Finally, the ratio of the posterior probabilities is known as the posterior odds.

```{r}
post.prob.h1 <- pbeta(0.05, shape1 = 1 + 1, shape2 = 1 + 100 - 1)
post.prob.h0 <- 1 - post.prob.h1
post.odds.h1 <- post.prob.h1 / post.prob.h0
post.odds.h1
```

The Bayes factor can be computed as the ratio of posterior odds and prior odds and equals 515.86.

```{r}
bf10 <- post.odds.h1 / prior.odds.h1
bf10
```

It is worth noting that this Bayes factor is remarkably high, considering the limited data that has been observed. However, this high value is not unexpected since the Bayes factor depends on the prior distribution of the parameters in the statistical model (in this case, $\theta$). Typically, when the prior distribution is very conservative, as is the case with the *beta(1, 1)* prior created with `method = 'default'`, the Bayes factor tends to overestimate the evidence in favor of the hypothesis of tolerable misstatement. To mitigate this dependence, you can use a prior distribution that is impartial towards the hypotheses by using `method = "impartial"` in the `auditPrior()` function [@Derks2022b].

```{r}
prior <- auditPrior(materiality = 0.05, method = "impartial", likelihood = "binomial")
evaluation(materiality = 0.05, x = 1, n = 100, prior = prior)
```

The analysis above was conducted using an impartial prior. The resulting output indicates that $BF_{10}=47$, which strongly supports $H_1$. This hypothesis suggests that the population contains misstatements lower than 5 percent (tolerable misstatement), assuming impartiality. Both prior distributions resulted in persuasive Bayes factors, making the results reliable regardless of the prior distribution selected. As a result, the auditor can confidently assert that the sample data provides convincing evidence that the population does not contain a material misstatement.

## Using a Data Set

Previously, we relied on summary statistics obtained from a sample to carry out evaluations. However, it is also possible to supply the `evaluation()` function with a data set. Doing so allows the function to calculate misstatements based on the booked and audited values of individual items.

To demonstrate how this works, we will use the `allowances` data set that comes with the **jfa** package. This data set includes 3500 financial statement line items, each with a booked value (`bookValue`) and an audited (true) value (`auditValue`) for illustrative purposes. Since this example focuses on the evaluation stage of an audit, the sample is already identified within the data set. For this example, the performance materiality has been set at 5 percent.

```{r}
data(allowances)
head(allowances)
```

When evaluating an audit sample using a data set, it is necessary to specify the `data`, `values`, and `values.audit` arguments in the function. The input for these arguments should be the name of the relevant column in `data`. The call below evaluates the `allowances` sample using a classical evaluation procedure. In this case, the output shows that the estimate of the misstatement in the population is 15.77 percent, with the 95 percent (one-sided) confidence interval ranging from 0 percent to 17.5 percent.

```{r}
x <- evaluation(materiality = 0.05, data = allowances, values = "bookValue", values.audit = "auditValue", times = "times")
summary(x)
```

## Exercises

1. Evaluate a sample of $n = 30$ items containing $k = 2$ misstatement. Use the classical approach.

<details>
<summary>Click to reveal answer</summary>
```{r}
evaluation(n = 30, x = 2, method = "binomial")
```
</details>

