```{r echo = FALSE, cache = FALSE}
source("utils.R", local = TRUE)
```

# Evaluation {#chap-evaluation}

In statistical audit sampling, the evaluation of the sample is the crucial last step in the process. Here, the auditor assesses the audit evidence collected from the sample and blends it with the evidence gathered in earlier stages of the audit. The outcome of this evaluation forms the auditor's overall opinion of the population being audited.

Just as with planning a sample, evaluating a sample requires knowledge of the circumstances that determine whether the population should be accepted or rejected, which are referred to as sampling objectives. Sampling objectives can be divided into two main categories:

- **Hypothesis testing**: The goal of the sample is to obtain evidence for or against the claim that the misstatement in the population is lower than a given value (i.e., the performance materiality).
- **Estimation**: The goal of the sample is to obtain an accurate estimate of the misstatement in the population with a certain precision.

![When evaluating a sample with respect to a specific hypothesis, the auditor must consider the evidence in favour as well as the evidence against that hypothesis. Image available under a [CC-BY-NC 4.0](https://creativecommons.org/licenses/by-nc/4.0/legalcode) license.](img/book_justice.png){#fig-im-eval fig-align="center" width=65%}

When an auditor needs to perform audit sampling, they typically have two options: non-stratified or stratified sampling. Non-stratified sampling involves selecting a sample of units without considering any categorical characteristics, such as product type or store location, of these items. Stratified sampling involves dividing the population into subgroups based on specific characteristics and selecting a sample from each subgroup. In comparison to non-stratified sampling, stratified sampling can improve efficiency. However, in this chapter, we will focus on non-stratified audit sampling, while stratified audit sampling will be covered in the next chapter.

Non-stratified sampling is typically used when the population is considered homogenous, meaning there are no significant differences between subgroups. This approach is also suitable when the auditor does not need to consider differences between subgroups. For example, when an auditor reviews a company's inventory using non-stratified sampling, they may choose a random sample of items from the entire inventory without dividing it into subgroups. Similarly, when conducting an audit of a small business's general ledger, an auditor may select a sample of entries without dividing them based on categorical features such as payment method.

To get started with non-stratified evaluation of audit samples, the auditor must first decide if they want to evaluate based on summary statistics derived from a sample (i.e., 1 misstatement in 100 items) or if they want to calculate the misstatements from a data set. To make the principle of evaluation easier to understand, we will first explain how to evaluate a sample using summary statistics from a sample.

## Classical Evaluation

In classical evaluation, confidence intervals and *p*-values are used to measure the uncertainty and the evidence against the hypothesis of intolerable misstatement, respectively.

Confidence intervals play a crucial role in classical inference by helping to determine the uncertainty in a sample estimate. For example, if an auditor needs to estimate the misstatement in a tax return, they can calculate a confidence interval for the misstatement using classical inference. This confidence interval represents a range of possible values in which the true misstatement of the population is likely to fall. This range helps auditors make informed decisions about the misstatement and determine the potential impact of the misstatement on the (loss of) taxes.

To illustrate, suppose an auditor wants to estimate the misstatement in a population based on a sample of 100 items containing one misstatement. Using the `evaluation()` function in **jfa** and specifying `x = 1` and `n = 100`, the output shows that the estimated most likely misstatement in the population is 1 percent, and the 95 percent (one-sided) confidence interval ranges from 0 percent to 4.74 percent. It is important to note that the correct interpretation of a 95 percent confidence interval is: *"If we were to repeat the experiment over and over, then 95 percent of the time the confidence interval contains the true misstatement rate"* [@Hoekstra2014].

```{r}
evaluation(x = 1, n = 100, method = "binomial")
```

Classical hypothesis testing relies on the *p*-value to determine whether to accept or reject a certain hypothesis about a population. For example, suppose an auditor wishes to test whether the population contains misstatements of less than 3 percent (they formulate the performance materiality based on existing rules and regulations). They would create the hypotheses $H_1:\theta<0.03$ and $H_0:\theta\geq0.03$. The significance level is set to 0.05, equivalent to an audit risk of 5 percent. This means that a *p*-value below 0.05 is sufficient to reject the hypothesis of intolerable misstatement $H_0$.

In **jfa**, a classical hypothesis test using the *p*-value can be conducted by specifying the `materiality` argument in the `evaluation()` function. For example, to indicate a performance materiality of 3 percent, the auditor can specify `materiality = 0.03`. Along with the confidence interval, the output displays a *p*-value of 0.19462, which is greater than 0.05. Therefore, the hypothesis $H_0$ cannot be rejected at a significance level of 5 percent. As a result, the auditor cannot conclude that the sample provides sufficient evidence to reduce the audit risk to an appropriate level and cannot state that the population does not have misstatements of 3 percent or more.

```{r}
eval <- evaluation(materiality = 0.03, x = 1, n = 100, method = "binomial")
eval
```

The exact definition of the *p*-value is *"the probability of observing the data, or more extreme data, given the truth of the hypothesis of intolerable misstatement"*. The *p*-value of 0.19462 can be visualized via the `plot()` function, see @fig-evaluation-freq-1.

```{r label="fig-evaluation-freq-1", fig.cap="The *p*-value is the sum of the observed and more extreme (but unobserved) outcomes, which in this case is the sum of $k=0$ and $k=1$ and equals 0.048 + 0.147 = 0.195."}
plot(eval)
```

## Bayesian Evaluation

In addition to classical evaluation methods, Bayesian inference offers an alternative approach to assessing audit samples. Unlike classical methods that use confidence intervals, Bayesian methods use credible intervals to measure the uncertainty in estimates.

Bayesian inference begins by specifying a prior distribution, which reflects prior knowledge about the misstatement in the population before any data is collected. This prior distribution is then combined with the information obtained from the sample to derive a posterior distribution. From the posterior distribution, credible intervals can be calculated to estimate the most likely misstatement in the population and the range of values within which the true value is likely to fall.

A Bayesian credible interval is intuitively interpreted as follows: *There is a 95 percent probability that the misstatement falls within the credible interval.* This is in contrast to the interpretation of a classical confidence interval, which is often misinterpreted for its Bayesian counterpart.

For instance, consider a scenario where a uniform *beta(1, 1)* prior distribution is used, along with a sample of 100 units, one of which contains a misstatement. Using the posterior distribution, it can be estimated that the most likely misstatement in the population is 1 percent. Furthermore, a Bayesian credible interval can be calculated to show that there is a 95 percent probability that the true misstatement rate lies between 0 percent and 4.61 percent. The small difference between the classical and default Bayesian results arises from the use of the uniform *beta(1, 1)* prior distribution. To achieve classical results, we can create a prior with `method = "strict"` using the `auditPrior()` function. Remember that any call to `evaluation()` can be done in a Bayesian way by specifying a prior distribution. Therefore, the sole difference between the call for a classical analysis and the call for a Bayesian analysis is the use of the `prior` constructed through a call to `auditPrior()`.

```{r}
prior <- auditPrior(method = "default", likelihood = "binomial")
eval <- evaluation(x = 1, n = 100, prior = prior)
eval
```

You can use the `plot()` function to visualize the posterior distribution along the most likely misstatement and the credible interval for the population misstatement. @fig-evaluation-bayes-1 shows this posterior distribution.

```{r label="fig-evaluation-bayes-1", fig.cap="The posterior distribution showing the most likely misstatement in the population as a gray dot and the 95% credible interval for the population misstatement as the black bars."}
plot(eval)
```

Bayesian hypothesis testing also involves the use of evidence measures, but instead of *p*-values, Bayesian inference employs the Bayes factor, either $BF_{10}$ or $BF_{01}$, to arrive at conclusions regarding the evidence furnished by the sample in favor of one of two hypotheses, $H_1$ or $H_0$. The Bayes factor quantifies the strength of evidence in favor of one hypothesis over another.

The Bayes factor provides an intuitive measure of statistical evidence, allowing auditors to interpret the probability of the data occurring under either hypothesis. For instance, if the `evaluation()` function outputs a value of 10 for $BF_{10}$, it means that the data are ten times more likely to have arisen under $H_1$ than under $H_0$. A Bayes factor $BF_{10}$ greater than 1 suggests evidence for $H_1$ and against $H_0$, while a Bayes factor $BF_{10}$ less than 1 suggests evidence for $H_0$ and against $H_1$. Although the `evaluation()` function returns $BF_{10}$ by default, one can compute $BF_{01}$ as the inverse of $BF_{10}$ (i.e., $\frac{1}{BF_{10}}$).

To illustrate, suppose an auditor wishes to verify whether a population contains less than 3 percent misstatement. Like before, this corresponds to the hypotheses $H_1:\theta<0.03$ and $H_0:\theta\geq0.03$. The auditor has taken a sample of 100 items, with only one containing a misstatement. By assuming a default *beta(1,1)* prior distribution, the following code evaluates the sample using a Bayesian hypothesis test and the Bayes factor. The `materiality = 0.03` argument specifies the materiality for this audit.

```{r}
evaluation(materiality = 0.03, x = 1, n = 100, prior = prior)
```

In this case, the Bayes factor is $BF_{10}=137.65$, which means that the sample data is 137.65 times more likely to occur under the hypothesis of tolerable misstatement than the hypothesis of material misstatement. We arrived at this value by considering both the prior distribution and the posterior distribution. Specifically, we first used the *beta(1,1)* prior distribution to calculate the prior probability of the hypothesis of tolerable misstatement.

```{r}
prior.prob.h1 <- pbeta(0.03, shape1 = 1, shape2 = 1)
prior.prob.h1
```

The probability of the hypothesis of intolerable misstatement is essentially the opposite of the probability of the hypothesis of tolerable misstatement. To clarify, it is just one minus the prior probability of the hypothesis of tolerable misstatement.

```{r}
prior.prob.h0 <- 1 - prior.prob.h1
prior.prob.h0
```

We use the prior probabilities to calculate the prior odds, which is the ratio of the prior probabilities.

```{r}
prior.odds.h1 <- prior.prob.h1 / prior.prob.h0
prior.odds.h1
```

To compute the posterior probability of the hypothesis of tolerable misstatement, we can use the posterior distribution and essentially follow the same steps. Hence, we calculate the posterior probability for the hypothesis of tolerable misstatement, then obtain the posterior probability of the hypothesis of intolerable misstatement by subtracting this probability from one. Finally, the posterior odds are calculated as the ratio of the posterior probabilities

```{r}
post.prob.h1 <- pbeta(0.03, shape1 = 1 + 1, shape2 = 1 + 100 - 1)
post.prob.h0 <- 1 - post.prob.h1
post.odds.h1 <- post.prob.h1 / post.prob.h0
post.odds.h1
```

Finally, the Bayes factor can be computed as the ratio of the posterior odds and the prior odds.

```{r}
bf10 <- post.odds.h1 / prior.odds.h1
bf10
```

It is worth noting that this Bayes factor of 137.65 is remarkably high, considering the data that has been observed. However, this high value is not unexpected since the Bayes factor depends on the prior distribution for $\theta$. Typically, when the prior distribution expresses a very conservative opinion on the population misstatement, as is the case with the *beta(1, 1)* prior, the Bayes factor tends to overestimate the evidence in favor of the hypothesis of tolerable misstatement. To mitigate this, you can use a prior distribution that is impartial towards the hypotheses by using `method = "impartial"` in the `auditPrior()` function [@Derks2022b].

```{r}
prior <- auditPrior(materiality = 0.03, method = "impartial", likelihood = "binomial")
evaluation(materiality = 0.03, x = 1, n = 100, prior = prior)
```

The analysis above was conducted using an impartial prior. The resulting output indicates that $BF_{10}=7.77$, which moderately supports $H_1$. This outcome suggests that the population contains misstatements lower than 5 percent (tolerable misstatement), assuming impartiality. Both prior distributions resulted in persuasive Bayes factors, making the results reliable regardless of the prior distribution selected. As a result, the auditor can confidently assert that the sample data provides evidence that the population does not contain a material misstatement.

## Using Data

Previously, we relied on summary statistics obtained from a sample to carry out evaluations. However, it is also possible to supply the `evaluation()` function with a data set. Doing so allows the function to calculate misstatements based on the booked and audited values of individual items.

To demonstrate how this works, we will use the `allowances` data set that comes with the **jfa** package. This data set includes 3500 financial statement line items, each with a booked value (`bookValue`) and an audited (true) value (`auditValue`) for illustrative purposes. Since this example focuses on the evaluation stage of an audit, the sample is already identified within the data set. For this example, the performance materiality has been set at 5 percent.

```{r}
data(allowances)
head(allowances)
```

When evaluating an audit sample using a data set, it is necessary to specify the `data`, `values`, and `values.audit` arguments in the `evaluation()` function. The input for these arguments should be the name of the relevant column in `data`. For example, the call below evaluates the `allowances` sample using a classical evaluation procedure. In this case, the output shows that the estimate of the misstatement in the population is 15.77 percent, with the 95 percent (one-sided) confidence interval ranging from 0 percent to 17.5 percent.

```{r}
x <- evaluation(materiality = 0.05, data = allowances, values = "bookValue", values.audit = "auditValue", times = "times")
summary(x)
```

### Stringer Bound

The Stringer bound is a commonly used method to evaluate audit samples. It is attractive because it takes into account the magnitude of the taints, thereby resulting in a smaller confidence interval (i.e., a lower upper bound). Note that, because it takes into account the magnitude of the taints, the Stringer bound only works if the actual data are present to calculate the taints.

Here we describe te calculation of the typical Stringer bound using the binomial distribution. $p(0; 1-\alpha)$ is the Clopper-Pearson one-sided upper conﬁdence bound for binomial parameter with 0 successes in $n$ trials which, for zero errors, can be calculated by $1 - \alpha^{\frac{1}{n}}$. The more general $p(j; 1-\alpha)$ is the Clopper-Pearson one-sided upper conﬁdence bound for binomial parameter with $j$ successes in $n$ trials. In other words, it is the proportion corresponding to a binomial distribution with $\alpha$\% chance that $j$ or less errors are observed in $n$ observations. That means that $p(j; 1-\alpha)$ is the unique solution of:

\begin{equation}
    \sum^n_{k = j + 1} {n \choose k} p^k (1-p)^{n-k} = 1 - \alpha
\end{equation}

The Stringer bound is calculated using the Clopper-Pearson bounds, the number of overstatements $m_+$ and the overstatement taints $z_+$.

\begin{equation}
    p(0; 1 - \alpha) + \sum_{j=1}^{m_+} \left[ p(j; 1 - \alpha) - p(j-1; 1 - \alpha) \right] \cdot z_{+_j}
\end{equation}

Note that the Stringer bound can also be calculated using the Poisson or hypergeometric distributions. The **jfa** package supports the Stringer bound in the `evaluation()` function using `method = "stringer.poisson"`, `stringer.binomial` or `stringer.hypergeometric`, depending on the preferred distribution.

```{r}
x <- evaluation(materiality = 0.05, data = allowances, method = "stringer.poisson", values = "bookValue", values.audit = "auditValue", times = "times")
summary(x)
```

As can be seen from the output, the upper bound is lower than that in the previous evaluation.

## Practical Exercises

1. Evaluate a sample of $n = 30$ items containing $k = 2$ misstatements. Use the classical approach.

::: {.content-visible when-format="html"}

<details>
<summary>Click to reveal answer</summary>
The evaluation can be performed using the `evaluation()` function with the default arguments.
```{r}
evaluation(n = 30, x = 2, method = "binomial")
```
</details>

:::

::: {.content-visible when-format="pdf"}

\clearpage

## Answers to the Exercises

1. The evaluation can be performed using the `evaluation()` function with the default arguments.

```{r}
evaluation(n = 30, x = 2, method = "binomial")
```
:::
