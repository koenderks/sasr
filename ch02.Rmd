---
output:
  bookdown::html_document2:
    fig_caption: yes
editor_options:
  chunk_output_type: console
---

```{r echo = FALSE, cache = FALSE}
source("utils.R", local = TRUE)
```

# Basic Concepts {#CHAPTER-2}

Statistical audit sampling is a statistical method used by auditors to evaluate
a sample of transactions or other items within a population in order to draw
conclusions about the population as a whole. It is a cost-effective way to test
the accuracy and reliability of financial information, as it allows auditors to
test a representative sample of the population rather than the entire population.

There are two main types of audit sampling: statistical sampling and
non-statistical sampling. Statistical sampling uses probability theory to
select a sample from the population and perform inferences about the population
based on the sample. Non-statistical sampling, on the other hand, is based on
the judgment of the auditor and does not involve statistical techniques. This
book does not concern itself with non-statistical sampling.

In statistical audit sampling, the auditor first defines the population and the
sampling units, which is the unit of measurement for the population
(e.g., transactions, invoices, etc.). The auditor then selects a sample from the
population using a specified sampling method, such as random sampling or
stratified sampling.

After the sample has been selected, the auditor performs audit procedures on the
sample and records the results. The auditor then uses statistical techniques to
calculate estimates of population characteristics, such as the mean or the
proportion of items with a certain characteristic (e.g., the proportion of
items that are misstated). The auditor compares these estimates to predetermined
acceptance criteria to determine (i.e., the materiality) whether the population
meets the criteria.

One important concept in statistical audit sampling is the confidence level. The
confidence level is the probability that the sample estimate falls within a
certain range of the true population value. For example, if the confidence level
is 95%, there is a 95% probability that the sample estimate is within the
specified range of the true population value. The confidence level is inversely
related to the audit risk $\alpha$, as follows:

$$\text{Confidence} = 1 - \text{Audit risk}(\alpha)$$

Hence, the desired confidence level is known to the auditor before they start
with their audit sampling activities.

Another important concept in statistical audit sampling is precision. Precision
refers to the degree to which the auditors' inferences are reliable. An estimate
with high precision is more likely to provide accurate conclusions about the
population. The precision of an estimate can be improved by increasing the
sample size.

To recap, the basic statistical concepts behind statistical audit sampling
include:

- **Population**: The entire group of items that the auditor is interested in
  studying. For example, the population may be all of the transactions in a
  company's accounts receivable.
- **Sampling unit**: A physical representation of the population to be audited.
- **Sample**: A subset of the population that is selected for testing. The
  sample should be representative of the population in order to accurately
  reflect the characteristics of the population.

In conclusion, audit sampling is a statistical method used by auditors to
evaluate a sample of transactions or other items within a population in order to
draw conclusions about the population as a whole. Statistical audit sampling
uses statistical techniques to select a sample from the population and make
inferences about the population based on the sample, while non-statistical
sampling is based on the judgment of the auditor.

## Frequentist versus Bayesian Inference

The Frequentist and the Bayesian approach to statistical inference are two
different ways to use data to perform inferences or draw conclusions about a
population. Both paradigms have their own set of assumptions and approaches to
statistical estimation and hypothesis testing.

Frequentist (classical) statistics is based on the concept of probability as a
frequency. It is the traditional approach to statistical analysis and relies on
the idea that probability is the long-term frequency of an event occurring in a
large number of trials. This approach is based on the law of large numbers,
which states that as the number of trials increases, the frequency of an event
will approach the true probability of the event occurring. In frequentist
estimation, the parameter that needs to be estimated is considered to be fixed,
but unknown. The goal of frequentist estimation is to use the sample data to
find the value of the parameter that is most likely to be true. This is
typically done using point estimates, which are single values that are thought
to be the best estimate of the population parameter, and interval estimates,
which provide a range of values that the population parameter is likely to fall
within.

Bayesian estimation, on the other hand, is based on the idea that the parameter
of interest is not fixed but uncertain. In this approach, the parameter is 
onsidered to be a random variable with a certain distribution, and the goal is
to use the data and prior knowledge about the parameter to update our belief
about its value. This is typically done using Bayes' theorem, which states that
the posterior probability (i.e., the updated belief about the parameter after
seeing the data) is equal to the prior probability (i.e., the belief about the
parameter before seeing the data) times the likelihood (i.e., the probability of
the data given the parameter).

One major difference between frequentist and Bayesian statistics is the way
they handle uncertainty. In frequentist statistics, uncertainty is represented
by the standard error of an estimate, which is a measure of the precision of an
estimate. In Bayesian statistics, uncertainty is represented by the posterior
distribution, which is a distribution of the possible values of the population
parameter given the sample data and our prior beliefs. Bayesian inferences
uses uses Bayes' theorem to update the prior beliefs about the population
parameter with the new information from the sample data. Bayes' theorem is given
by the following formula:

$$p(\theta \,|\, y) = \frac{p(y \,|\, \theta)p(\theta)}{p(y)}$$

where $p(\theta \,|\, y)$ is the posterior probability of the population
parameter $\theta$ given the sample data $y$, $p(y \,|\, \theta)$ is the
likelihood of the sample data given $\theta$, $p(\theta)$ is the prior
probability of $\theta$, and $p(y)$ is the total probability of the sample data
occurring. Because with a fixed sample $p(y)$ is a constant, Bayes' theorem is
often given as follows:

$$p(\theta \,|\, y) \propto p(y \,|\, \theta) \times p(\theta)$$

or, in words:

$$\text{Posterior} \propto \text{Likelihood} \times \text{Prior}$$

There are several other differences between frequentist and Bayesian approaches.
One major difference is that frequentist approaches focus on the long-run
behavior of an estimator, while Bayesian approaches focus on the probability of
the estimator being correct given the data. Another difference is that
frequentist approaches rely on asymptotic results (i.e., results that hold in
the limit as the sample size goes to infinity), while Bayesian approaches can be
applied with any sample size. Additionally, Bayesian approaches allow for the
incorporation of prior knowledge about the parameter, while frequentist
approaches do not.

Overall, frequentist and Bayesian statistics are two different approaches to
statistical analysis that have their own unique strengths and weaknesses.
Frequentist statistics is a well-established approach that is widely used in
scientific research and audit practice, but it has some limitations, such as its
reliance on assumptions and its inability to incorporate prior beliefs. Bayesian
statistics is a newer approach that allows for more flexibility in statistical
inferences, but it requires the specification of prior beliefs, which can be
difficult to quantify.
