---
output:
  bookdown::html_document2:
    fig_caption: yes
editor_options:
  chunk_output_type: console
---

```{r echo = FALSE, cache = FALSE}
source("utils.R", local = TRUE)
```

# Planning {#CHAPTER-PLANNING}

<p align='center'><img src='https://github.com/koenderks/jfa/raw/development/vignettes/img/planning.png' alt='planning' width='100%'></p>

One of the key considerations in audit sampling is determining the minimum
sample size required to achieve a desired level of assurance or precision. In
this chapter, we will discuss how to use three standard likelihoods to plan a
minimum sample size for audit sampling: the hypergeometric likelihood, the
binomial likelihood and the Poisson likelihood.

Before we dive into the details of computing a minimum sample size for audit
sampling using any distribution, it is important to review some key concepts and
notation. Let's start by defining the population and the sample. The population
is the group of items or transactions being audited, and the sample is the
subset of the population that is selected for testing. The population size is
denoted by $N$, and the sample size is denoted by $n$.

Next, we need to define the maximum tolerable error or precision level that we
want to achieve in our audit. This is typically expressed as a maximum allowable
misstatement rate, denoted by $\theta_{max}$. For example, if we set
$\theta_{max}$ to 0.05, it means that we want to ensure that the misstatement
rate in the population does not exceed 5%.

## Required Information

First, planning a minimum sample requires knowledge of the conditions that lead 
to acceptance or rejection of the population (i.e., the sampling objectives).
Typically, sampling objectives can be classified into one or both of the
following:

- **Hypothesis testing**: The goal of the sample is to obtain evidence for or
  against the claim that the misstatement in the population is lower than a
  given value (i.e., the performance materiality).
- **Estimation**: The goal of the sample is to obtain an accurate estimate of
  the misstatement in the population (with a minimum precision).

Second, it is advised to specify the expected (or tolerable) misstatements in
the sample. The expected misstatements are the misstatements that you allow in
the sample, while still retaining the desired amount of assurance about the 
population. It is strongly recommended to set the value for the expected
misstatements in the sample conservatively to minimize the chance of the
observed misstatements in the sample exceeding the expected misstatements, which
would imply that insufficient work has been done in the end.

Next to determining the sampling objective(s) and the expected misstatements, it
is also important to determine the statistical distribution linking the sample
outcomes to the population misstatement. This distribution is called the
likelihood (i.e., `poisson`, `binomial`, or`hypergeometric`). All three
aforementioned likelihoods are commonly used in an audit sampling context,
however, `poisson` is the default likelihood in **jfa** because it is the most
conservative of the three.

## Hypergeometric Distribution

Let's consider how to use the hypergeometric distribution to calculate the
minimum sample size needed to achieve a desired level of assurance. The
hypergeometric distribution is a discrete probability distribution that is
commonly used to model the number of events occurring in a fixed number of
trials when the population size is known. For our purpose, we can use the
hypergeometric distribution to model the number of misstatements that are
expected to be found in the sample.

The probability mass function (PMF) of the hypergeometric distribution is given
by:

$$p(X=x)=\frac{\binom{K}{x}\binom{N-K}{n-x}}{\binom{N}{n}}$$

where $x$ is the number of misstatements in the sample, $n$ is the sample size,
$N$ is the population size and $K$ is the total number of misstatements assumed
in the population. The value of $K$ is a function of the specified
$\theta_{max}$:

$$K = \theta_{max} N$

Hence, given a desired misstatement tolerance $\theta_{max}$, we can solve for
the minimum sample size $n$ needed to achieve this assurance level. In **jfa**,
this sample size can be calculated using the `planning()` function. For example,
if we want to achieve an assurance level of $\alpha=0.05$ for a
maximum tolerable misstatement rate of 3% in a population of $N=1000$, then the
required sample size under the assumption of zero expected misstatements in the
sample is $n = 94$.

```{r}
planning(min.precision = 0.03, expected = 0, conf.level = 0.95, likelihood = "hypergeometric", N.units = 1000)
```

As another example, if we want to achieve an assurance level of $\alpha=0.05$
for a maximum tolerable misstatement rate of 3% in a population of $N=1000$,
then the required sample size under the assumption of one expected misstatement
in the sample is $n = 115$.

```{r}
planning(min.precision = 0.03, expected = 1, conf.level = 0.95, likelihood = "hypergeometric", N.units = 1000)
```

## Binomial Distribution

Let's consider how to use the binomial distribution to calculate the minimum
sample size needed to achieve a desired level of assurance. The binomial
distribution is a discrete probability distribution that is commonly used to
model the number of events occurring in a fixed number of trials. For our
purpose, we can use the binomial distribution to model the number of
misstatements that are expected to be found in the sample. In an audit sampling
context, the binomial distribution is often used to approximate the
hypergeometric distribution since it is easier to work with. However, the
binomial distribution is also more conservative than the hypergeometric
distribution, meaning that resulting sample sizes will be higher.

The probability mass function (PMF) of the binomial distribution is given by:

$$p(x; n, \theta) = \binom{n, x} \theta^{x} (1-\theta)^{n - x}$$

where $x$ is the number of misstatements in the sample, $n$ is the sample size
and $\theta$ is the misstatement rate expected in the sample.

Hence, given a desired misstatement tolerance $\theta_{max}$, we can solve for
the minimum sample size $n$ needed to achieve this assurance level. A useful
trick to utilize is that, if we do not expect any misstatements in the sample,
the formula for the required sample size reduces to:

$$n = \ceil{\frac{\ln(\alpha)}{\ln(1 - \theta_{max})}}$$.

For example, if we want to achieve an assurance level of $\alpha=0.05$ for a
maximum tolerable misstatement rate of 3%, then the required sample size under
the assumption of zero expected misstatements in the sample is $n = 99$.

```{r}
ceiling(log(1 - 0.95) / log(1 - 0.03))
```

In **jfa**, this sample size can be replicated using the `planning()` function.

```{r}
planning(min.precision = 0.03, expected = 0, conf.level = 0.95, likelihood = "binomial")
```

However, if the number of expected misstatements in the sample is non-zero,
it becomes more difficult to solve the formula for $n$. Hence, we can
iteratively try every value of $n$ and return the smallest integer that
satisfies the sampling objectives. In **jfa**, this can be done by adjusting the
`expected` argument in the `planning()` function. For example, if we want to
achieve an assurance level of $\alpha=0.05$ for a maximum tolerable misstatement
rate of 3%, then the required sample size under the assumption of one expected
misstatement in the sample is $n = 123$.

```{r}
planning(min.precision = 0.03, expected = 1, conf.level = 0.95, likelihood = "binomial")
```

## Poisson Distribution

Let's consider how to use the Poisson distribution to calculate the minimum
sample size needed to achieve a desired level of assurance. The Poisson
distribution is a discrete probability distribution that is commonly used to
model the number of events occurring in a fixed time or space. We can use the
Poisson distribution to model the number of misstatements that are expected to
be found in the sample. In an audit sampling context, it is often used to
approximate the binomial distribution since it is easier to work with. However,
the Poisson distribution is also more conservative than the binomial
distribution, meaning that resulting sample sizes will be higher.

The probability mass function (PMF) of the Poisson distribution is given by:

$$p(x;\lambda) = \frac{\lambda^x e^{-\lambda}}{x!}$$

where $x$ is the number of misstatements in the sample, and $\lambda$ is the
average number of misstatements expected in the sample. The average number of
misstatements is related to the misstatement rate in the population, denoted by
$\theta$, and the sample size, $n$, by the following equation:

$$\lambda=n\theta$$.

Hence, given a desired misstatement tolerance $\theta_{max}$, we can solve for
the minimum sample size $n$ needed to achieve this assurance level. A useful
trick to utilize is that, if we do not expect any misstatements in the sample,
the formula for the required sample size reduces to:

$$n = \ceil{-\frac{\ln(\alpha)}{\theta_{max}}}$$.

For example, if we want to achieve an assurance level of $\alpha=0.05$ for a
maximum tolerable misstatement rate of 3%, then the required sample size under
the assumption of zero expected misstatements in the sample is $n = 100$.

```{r}
ceiling(-log(1 - 0.95) / 0.03)
```

In **jfa**, this sample size can be replicated using the `planning()` function.

```{r}
planning(min.precision = 0.03, expected = 0, conf.level = 0.95, likelihood = "poisson")
```

However, if the number of expected misstatements in the sample is non-zero,
it becomes more difficult to solve the formula for $n$. Hence, we can
iteratively try every value of $n$ and return the smallest integer that
satisfies the sampling objectives. In **jfa**, this can be done by adjusting the
`expected` argument in the `planning()` function. For example, if we want to
achieve an assurance level of $\alpha=0.05$ for a maximum tolerable misstatement
rate of 3%, then the required sample size under the assumption of one expected
misstatement in the sample is $n = 125$.

```{r}
planning(min.precision = 0.03, expected = 1, conf.level = 0.95, likelihood = "poisson")
```

## Bayesian Planning

Performing Bayesian planning requires an input for the `prior` argument in the
`planning()` function. Setting `prior = TRUE` performs Bayesian planning using a
[default prior](https://koenderks.github.io/jfa/articles/creating-prior.html#default-priors-method-default)
conjugate to the specified `likelihood`. For example, the command below uses a
default gamma(1, 1) prior distribution to plan the sample, since `planning()`
defaults to the Poisson likelihood.

```{r}
plan <- planning(materiality = 0.05, expected = 0, conf.level = 0.95, prior = TRUE)
summary(plan)
```

You can inspect how the prior distribution compares to the expected
posterior distribution by using the `plot()` function. The expected posterior
distribution is the posterior distribution that would occur if you actually
observed the planned sample containing the expected misstatements.

```{r fig.align="center", fig.height=4, fig.width=6}
plot(plan)
```

The input for the `prior` argument can also be an object created by the
`auditPrior` function. If `planning()` receives a prior for which there is a
conjugate likelihood available, it will inherit the likelihood from the prior.
For example, the command below uses a custom normal(0, 0.1) prior distribution to
plan the sample using the binomial likelihood.

```{r}
prior <- auditPrior(method = "param", likelihood = "normal", alpha = 0, beta = 0.1)
planning(materiality = 0.05, expected = 0, conf.level = 0.95, prior = prior)
```

